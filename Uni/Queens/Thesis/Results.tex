\chapter{Results}

Having constructed the theory of the physics and computation, now the results that come about from these reconstruction efforts can be covered. To discuss these results, consider the methods in which a reconstruction technique can be considered successful. The easiest, and most useful parameter to check is the direction. Comparing the reconstructed direction with the true direction is done by computing the solid angle between the directions. Computing this is simple; suppose the reconstructed direction is $\vec{u}$ and the true direction is $\vec{v}$, then the solid angle is
\begin{equation}
  \alpha = \arccos(\vec{u}\cdot\vec{v})\, ,
\end{equation}
where the assumption that the directions are normalized to unit length is made. This solid angle can then be used to fill a histogram of a large number of trials to build a distribution. 

\section{Linefit}

The simplest method for reconstruction that has been discussed is the linear fit using the method of least squares, interchangably referred to as the `linefit'. In figure \ref{fig:alpha_linefit}, it is seen that this method provides a pretty decent first guess for a direction. The angular resolution seems to peak around a three degree resolution, and slowly falls off further away from the truth. 

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/alpha_dist_linefit_norm.eps}
  \caption{A distribution of the solid angles for reconstructed and true directions using the linefit method. The solid angle is given in degrees for clarity.}
  \label{fig:alpha_linefit}
\end{figure}

The quality of this reconstruction will inevitably affect the quality of the following reconstruction step, and as such should not be ignored. 

\section{Likelihood}

It is easy to see that a similar plot for the angular distribution of the reconstructed angle using the linefit as a seed to the likelihood algorithm can be made. Looking at figure \ref{fig:alpha_llh} it can be noted how well the likelihood reconstruction performs when using the linefit as a seed, but also how it compares to a perfect starting point, the true track information.

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/alpha_dist_llh_norm.eps}
  \caption{A distribution of the solid angles for reconstructed and true directions using the likelihood method. The linefit reconstruction angular resolution is also plotted alongside a distribution for the likelihood given the true track as a starting point.}
  \label{fig:alpha_llh}
\end{figure}

Figure \ref{fig:alpha_llh} shows us that with perfect information, the likelihood reconstruction does pretty well. This effectively shows the upper limit to the performance of the reconstruction as it is, but also shows that there is plenty of room for improvement when starting at the linefit seed. It is relatively standard for reconstruction techniques to be highly dependent upon the initial conditions, as the technology used boils down to being a minimization of a multiparameter space. This effect is in full swing when the two starting conditions are compared, which emphasizes that another path to improving the reconstruction would be to improve the initial guess.

This resolution plot can also be represented by plotting the difference in the directional coordinates. As the directions are length one, seperate the azimuthal ($\phi$) and zenith ($\theta$) errors. Then, plotting the difference between the true azimuth/zenith and the reconstructed azimuth/zenith leads to figure \ref{fig:alpha_llh_sep}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/angular_diff_dir.eps}
  \caption{A distribution of the solid angles for reconstructed and true directions using the likelihood method seperated into zenith and azimuth components in degrees. \textbf{Above:} The zenith component distribution where $\Delta\theta = \theta_{\text{true}} - \theta_{\text{reco}}$ in the reconstruction process. \textbf{Below:} The azimuth component distribution where $\Delta\phi = \phi_{\text{true}} - \phi_{\text{reco}}$ in the reconstruction process.}
  \label{fig:alpha_llh_sep}
\end{figure}

The reconstruction still shows a similar shape that that observed in figure \ref{fig:alpha_llh}, as clearly the angular resolution in highly initial condition dependent. There is something of note in the zenth plot though ($\theta$), as there is a slight skewing towards negative $\Delta\theta$ values when using the linefit. This is due to the way the detector is simulated, since the current setup assumes IceCube-like DOMs which only face downward. As linefit is ignorant to the actual method in which light is emmitted from the track, this introduces a degeneracy in the fit that can push linefit towards a particular direction.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|} 
    \hline
    \textbf{Angle} & \multicolumn{2}{|c|}{$\bm{\Delta\theta(^{\circ})}$} & \multicolumn{2}{|c|}{$\bm{\Delta\phi(^{\circ})}$} \\
    \hline\hline
    \textbf{Statistic} & $\bm{\mu}$ & $\bm{\sigma_{s}}$ & $\bm{\mu}$ & $\bm{\sigma_{s}}$ \\
    \hline\hline
    \textbf{line} & -1.89 & 7.21 & -0.01 & 9.52 \\
    \hline
    \textbf{default} & -1.47 & 6.32 & -0.002 & 8.41 \\
    \hline
    \textbf{true} & -0.28 & 2.74 & -0.62 & 4.18 \\
    \hline
  \end{tabular}
  \caption{Some simple statistics on the data distributions in Figure \ref{fig:alpha_llh_sep}. Here `\textbf{line}', `\textbf{default}' and `\textbf{true}' refer to just the linear fit, the likelihood using the linear fit as the seed, and the likelihood using the truth as a seed fits respectively. The means ($\bm{\mu}$) are computed in the standard way, but the standard deviation ($\bm{\sigma_{s}}$) is the sample standard deviation, and hence is normalized by a factor dependent upon the sample size. }
  \label{tab:angular_diff}
\end{table}

The resolution in the zenith angle seems to be slightly better than that of the azimuthal, which can be attributed to the geometry of the detector. In particular, the current geometry assumes vertical lines of detectors, which would expectedly perform better. These plots are in fact excellent for attempting to understand the effects of the geometry of the detector. Referring to Table \ref{tab:angular_diff} the difference in the accuracy of the azimuthal to the zenith becomes clear. The zenith has a consistently reduced sample standard deviation over all three methods of fitting, with subsequent techniques improving on the previous. The interesting values to note are all of the means being negative. These negatives aren't as substantial for the values that are closer to zero, like the means in $\Delta\phi$. The means in $\Delta\theta$ do tend closer to zero as the techniques ``improve'', however the substantial shift in the linefit mean is an artifact of the method of simulation used. As the DOMs have been simulated in a manner similar to that of IceCube, they are all downward facing and bias the linear fit to be consistently fit more horizontally than the truth, as a negative sign implies the reconstructed azimuthal is larger than the true azimuthal. This bias should in theory vanish if upward facing DOMs were to be included, but that is something to be considered as a part of the detector geometry.

\subsection{Seed Comparisons}

To test which initial parameter most affects the reconstruction, start the reconstruction seed from two different initial states; fixing the starting vertex at the truth, and fixing the starting direction in the true direction. Using these initial conditions, a plot similar to figure \ref{fig:alpha_llh} could be created, and is in figure \ref{fig:alpha_llh_test}. As can be seen from the legend, there are a couple of variable seeds in this distribution, and the results seem to point towards the vertex resulting in the largest improvement in the resolution. This can come off as a bit surprising, especially when the parameter that is being plotted against is the direction resolution. The improvement is informative as to where the initial guess needs to be improved, and focusing on an improved vertex for the initial seed seems to be the way to go.

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/alpha_dist_llh_seedcomparison.eps}
  \caption{Similar to the distribution in figure \ref{fig:alpha_llh}, the angular resolution is plotted in degrees into a histogram. Here the initial starting conditions are grouped into the true vertex, true direction and only linefit, where the former are given linefit parameters for the remaining parameters.}
  \label{fig:alpha_llh_test}
\end{figure}

To get a better idea of how the plots in Figure \ref{fig:alpha_llh_test} and Figure \ref{fig:alpha_llh} compare, a simple table to look at the percentage of events that are below certain cut-off angular resolution values was made. In Table \ref{tab:alpha_comp} these exact percentages for angular resolutions below $0.1^{\circ}$, $0.5^{\circ}$, $1^{\circ}$, $5^{\circ}$, and $10^{\circ}$ from the true direction are noted. Comparing the values, the largest relative increases in the percentages seem to occur between linefit and the standard likelihood reconstruction and going to the likelihood reconstruction using the truth as a seed. Looking at the resolutions below $0.1^{\circ}$, $0.5^{\circ}$, and $1^{\circ}$ the percentage of events increase by an order of magnitude between linefit and the standard likelihood method. This same jump occurs again when getting to the likelihood reconstruction using the truth as a seed. 

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|} 
    \hline
    $\mathbf{\alpha}$ & $\mathbf{0.1^{\circ}}$ & $\mathbf{0.5^{\circ}}$ & $\mathbf{1^{\circ}}$ & $\mathbf{5^{\circ}}$ & $\mathbf{10^{\circ}}$ \\
    \hline
    \textbf{line} & 0.02 & 0.39 & 1.54 & 24.25 & 50.76 \\
    \hline
    \textbf{llh} & 0.47 & 5.67 & 11.43 & 38.02 & 59.58 \\
    \hline
    \textbf{llhdir} & 0.69 & 7.60 & 15.70 & 50.11 & 70.54 \\
    \hline
    \textbf{llhvert} & 0.71 & 8.99 & 19.23 & 57.65 & 76.31 \\
    \hline
    \textbf{llhtrue} & 2.70 & 25.03 & 45.21 & 86.92 & 96.29 \\
    \hline
  \end{tabular}
  \caption{The labels `\textbf{line}', `\textbf{llh}', `\textbf{llhdir}', `\textbf{llhvert}', `\textbf{llhtrue}' refer to the linefit, likelihood, likelihood using the true direction seed, likelihood using the true vertex seed and likelihood using the truth as a seed reconstructions respectively. The values are given in percentage of events below the given $\alpha$ solid angle to the true direction.} 
  \label{tab:alpha_comp}
\end{table}

As the largest increase in accuracy occurs from changing the vertex position, as seen in Table \ref{tab:alpha_comp}, a comparison of the vertex positions given by the linear fit with the true vertex positions might provide some insight into why this is so. This is a non-trivial comparison, as the vertex position has three free variables, and projecting along a single axis isn't particularly useful due to the nature of the linear fit vertex ocassionaly ending up very far away from the detector. For this reason, a useful representation is one similar to that of Figure \ref{fig:alpha_llh_sep}, as in the process of reconstructing, the vertex is projected onto a sphere anyways. Thus, assuming this projection of the vertices, the problem reduces to looking at the angle distribution is the zenith ($\theta$) and azimuth ($\phi$). Figure \ref{fig:vert_dist} shows what this distribution looks like compared with the true vertex distribution. 

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/angular_vert_dist_modified.eps}
  \caption{A distribution of the vertex zenith and azimuthal components when projected onto a sphere of radius 550 meters. \textbf{Above:} The zenith component distribution. \textbf{Below:} The azimuth component distribution.}
  \label{fig:vert_dist}
\end{figure}

The shapes of the plots in Figure \ref{fig:vert_dist} are telling. The top plot, the zenith distribution, is heavily pushed up to angles around zero for good reason. The simulated muons are primarily cosmic, and hence the vertex being primarily in the $[0, 50]$ degree range makes sense. The lack of angles in the symmetric part near 360 are due purely to the branch choice as generally for spherical coordinates $\theta \in [0,180]$ degrees since the complement angle is covered by the azimuthal angle. The more valuable point is the difference in the linear fit zenith distribution with the true distribution. The true distribution is consistent with a cosmic muon source, but the linear fit seems to be dragging the vertex positions towards two particular peaks. These peaks can be seen in the zoomed plot made in Figure \ref{fig:vert_dist_zoomed} to be around 10 and 22 degrees. These angular peaks can be explained as the vertices tending to bunch up near the DOM locations. If the source of muons was isotropic, and not just cosmic, one could expect this pattern to repeat throughout the domain of the zenith. To understand why these peaks occur, it is simplest to think of a zenith distribution as a projection along the azimuthal angle. Unlike simple cartesian projections, which are usually onto a plane in three dimensions, this is a projected onto a rotating surface. The result is a stacking of the azimuthal dependence, leaving only the distance between the DOMs. 

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/zenith_vert_dist_modified_zoomed.eps}
  \caption{A distribution of the vertex zenith and azimuthal components when projected onto a sphere of radius 550 meters. \textbf{Above:} The zenith component distribution. \textbf{Below:} The azimuth component distribution.}
  \label{fig:vert_dist_zoomed}
\end{figure}

On the other hand, the azimuthal spans its full range of $[0, 360]$ degrees. The true vertex azimuth can be observed as being relatively uniform, yet the linefit reconstructed vertex has a clear oscillatory pattern. Similar to the peaks observed in the zenith distribution, these can be attributed to the DOMs where here the projection along the zenith leads to the distance between the lines of the DOMs being the active agent. Both the azimuthal and zenith distributions show that the linear fit has an inherent bias towards the locations of the optical modules, and removing this bias could improve the vertex reconstruction which would in turn improve the likelihood reconstruction as seen in Figure \ref{fig:alpha_llh_test}.

\subsection{Correlations}

Next, it can be checked whether there is a correlation with the likelihood values and the reconstructed angular resolution ($\alpha$). Both the final and initial likelihood values need to be included, as there can be a wide variety of ranges that a reconstruction can start and end at. For this reason, consider plotting the ratio between the final negative loglikelihood value and the initial, $\ell_{f}/\ell_{i}$. Thus, a smaller value denotes a larger improvement in the likelihood value. On the other hand, any value above one denotes a drop in quality of fit. This plot is made in figure \ref{fig:alpha_llhratio_comp}.

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/alpha_dist_vs_llhratio_heat.png}
  \caption{We plot the likelihood ratio $\ell_{f}/\ell_{i}$ against the final reconstructed angular resolution using a heatmap. }
  \label{fig:alpha_llhratio_comp}
\end{figure}

Referring to figure \ref{fig:alpha_llhratio_comp}, note that the events are bunching up near a small ratio of $\ell_{f}/\ell_{i}$ and small error in the solid angle $\alpha$. This could point to a correlation between the increase in the quality of the fit and the increase in the direcitonality of the reconstructed track. There could still be some hidden biases here that are being unaccounted for, such as reconstruction angles that begin close to the truth only changing in small amounts but having massive likelihood value swings. For this reason, it needs to be ensured that there are no hidden biases in both $\ell_{f}$ against $\ell_{i}$ and $\alpha_{f}$ against $\alpha_{i}$.

\begin{figure}[ht]
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{./Figures/reco_plots/llhratio_comp_heat.png}
    \caption{Correlation heatmap between the initial negative loglikelihood value and the final negative loglikelihood.}
    \label{subfig:llh_heat}
  \end{minipage}
  \hspace{0.1cm}
  \begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{./Figures/reco_plots/alpha_dist_comp_heat.png}
    \caption{Correlation map of the final reconstructed angular resolution and the initial angular resolution.}
    \label{subfig:alpha_heat}
  \end{minipage}
\end{figure}

Looking at figures \ref{subfig:llh_heat} and \ref{subfig:alpha_heat}, the reconstruction is behaving as one would expect. For both plots, the $y=x$ diagonal line would symbolize neither improvement nor regression, and anything below would suggest improvement. In both figures majority of the points end up below this line encouraging the correlation between the reconstruction improving both the likelihood value but also the reconstructed angular direction. For figure \ref{subfig:llh_heat}, the clustering of events shows a correlation between good seed events leading to well fit final fits. The further the seed events get from being good fits, the more spread the quality of the final fit becoms. A similar result can be seen with the angular resolution correlation plot in figure \ref{subfig:alpha_heat}. A useful method to quantify these correlations is the Pearson Correlation, where a value of one is perfect correlation, and a value of negative one is of perfect negative correlation. Figure \ref{subfig:llh_heat} gives a Pearson Correlation of 0.38 while Figure \ref{subfig:alpha_heat} gives a correlation of 0.93. These are interesting values as at first glance it may seem that the likelihood values have a clearer correlation than the solid angles, but the correlation values seem to tell another story. There is still clearly a positive correlation, but the reduced correlation value in the likelihoods is interesting, however the observations from before still stand as there are plenty of events that occur below the $y=x$ line. 

\subsection{Computation Time}

Another important parameter to consider when looking at reconstructions is the time it takes to run. This can become increasingly important as there are massive amounts of potential events a detector can observe during its runtime, and being able to reconstruct them becomes an important potential bottleneck. 

\begin{figure}[H]
  \centering
  \includegraphics[width=12cm]{./Figures/reco_plots/computation_time_perpulse.eps}
  \caption{Distribution of the computation time for reconstructing events normalized by the pulse count. Higher energy events will naturally need to a larger number of observed pulses and increase the computation time accordingly. }
  \label{fig:comp_time}
\end{figure}

Figure \ref{fig:comp_time} plots the computation time for the reconstruction algorithm normalized by the pulse count, where the pulse count scales with the number of photons produced, and hence the energy of the muon. The peak occurs around 0.125 sec/pulse, which isn't great, though it is a decent first efficiency test. Ideally this will be sped up either by using clever techniques, or by shifting to a language like C++ where this repetative nature of nested loops is better optimized. Parallelization schemes could also be introduced if even more speed up is required. 


